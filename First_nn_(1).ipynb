{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d466826f",
      "metadata": {
        "id": "d466826f",
        "outputId": "f9f2ae8d-8bf8-4894-ba79-c94c72d1c2a4"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "try {\n",
              "require(['notebook/js/codecell'], function(codecell) {\n",
              "  codecell.CodeCell.options_default.highlight_modes[\n",
              "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
              "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
              "      Jupyter.notebook.get_cells().map(function(cell){\n",
              "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
              "  });\n",
              "});\n",
              "} catch (e) {};\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "try {\n",
              "require(['notebook/js/codecell'], function(codecell) {\n",
              "  codecell.CodeCell.options_default.highlight_modes[\n",
              "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
              "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
              "      Jupyter.notebook.get_cells().map(function(cell){\n",
              "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
              "  });\n",
              "});\n",
              "} catch (e) {};\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pynq.overlays.base import BaseOverlay\n",
        "\n",
        "base = BaseOverlay(\"base.bit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5712d2d",
      "metadata": {
        "id": "b5712d2d"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d28e2210",
      "metadata": {
        "id": "d28e2210"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import os\n",
        "from urllib import request\n",
        "from struct import unpack\n",
        "from random import randint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6537593e",
      "metadata": {
        "id": "6537593e"
      },
      "outputs": [],
      "source": [
        "def read_images(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        magic, num_images, rows, cols = unpack('>IIII', f.read(16))\n",
        "        images = np.fromfile(f, dtype=np.uint8).reshape(num_images, rows, cols, 1)\n",
        "    return images\n",
        "\n",
        "def read_labels(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        magic, num_labels = unpack('>II', f.read(8))\n",
        "        labels = np.fromfile(f, dtype=np.uint8)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2785985",
      "metadata": {
        "id": "c2785985"
      },
      "outputs": [],
      "source": [
        "def load_mnist():\n",
        "    data_dir = \"MNIST_data\"\n",
        "    files = ['train-images.idx3-ubyte', 'train-labels.idx1-ubyte',\n",
        "             't10k-images.idx3-ubyte', 't10k-labels.idx1-ubyte']\n",
        "    datasets = []\n",
        "    for file in files:\n",
        "        path = os.path.join(file)\n",
        "        datasets.append(path)\n",
        "    X_train = read_images(datasets[0])\n",
        "    y_train = read_labels(datasets[1])\n",
        "    X_test = read_images(datasets[2])\n",
        "    y_test = read_labels(datasets[3])\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c44f6c",
      "metadata": {
        "id": "d3c44f6c"
      },
      "outputs": [],
      "source": [
        "def conv2d(X, kernel, bias, stride=1, padding=0):\n",
        "    if len(X.shape) == 3:\n",
        "        input_height, input_width, input_channels = X.shape\n",
        "    elif len(X.shape) == 2:\n",
        "        # If the input is a single-channel image, add a singleton dimension for channels\n",
        "        input_height, input_width = X.shape\n",
        "        input_channels = 1\n",
        "        X = X.reshape(input_height, input_width, 1)\n",
        "    else:\n",
        "        raise ValueError(\"Input data must be 2D or 3D array\")\n",
        "\n",
        "    kernel_height, kernel_width, _, num_filters = kernel.shape\n",
        "    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n",
        "    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n",
        "    output = np.zeros((output_height, output_width, num_filters))\n",
        "\n",
        "    X_padded = np.pad(X, ((padding, padding), (padding, padding), (0, 0)), mode='constant')\n",
        "\n",
        "    for f in range(num_filters):\n",
        "        for i in range(0, output_height):\n",
        "            for j in range(0, output_width):\n",
        "                output[i, j, f] = np.sum(X_padded[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width, :] * kernel[:, :, :, f]) + bias[:, :, :, f]\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d81201e",
      "metadata": {
        "id": "1d81201e"
      },
      "outputs": [],
      "source": [
        "def max_pooling(X, pool_size=(2, 2), stride=2):\n",
        "    input_height, input_width, input_channels = X.shape  # Update to handle 3D array\n",
        "    pool_height, pool_width = pool_size\n",
        "    output_height = (input_height - pool_height) // stride + 1\n",
        "    output_width = (input_width - pool_width) // stride + 1\n",
        "    output = np.zeros((output_height, output_width, input_channels))  # Update output shape\n",
        "    for i in range(output_height):\n",
        "        for j in range(output_width):\n",
        "            for k in range(input_channels):  # Loop over channels\n",
        "                output[i, j, k] = np.max(X[i * stride:i * stride + pool_height, j * stride:j * stride + pool_width, k])\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "743e7a39",
      "metadata": {
        "id": "743e7a39"
      },
      "outputs": [],
      "source": [
        "# Function to perform ReLU activation\n",
        "def relu(X):\n",
        "    return np.maximum(X, 0)\n",
        "\n",
        "# Function to perform softmax activation\n",
        "def softmax(X):\n",
        "    exp_vals = np.exp(X - np.max(X))\n",
        "    return exp_vals / np.sum(exp_vals)\n",
        "\n",
        "def initialize_parameters():\n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    parameters['W1'] = np.random.randn(3, 3, 1, 8) * 0.1\n",
        "    parameters['b1'] = np.zeros((1, 1, 1, 8))\n",
        "    parameters['W2'] = np.random.randn(3, 3, 8, 16) * 0.1\n",
        "    parameters['b2'] = np.zeros((1, 1, 1, 16))\n",
        "\n",
        "    # Compute the size of the flattened output from the second convolutional layer\n",
        "    flattened_size = 7 * 7 * 16  # Assuming pooling with pool size (2, 2) and stride 2\n",
        "\n",
        "    parameters['W3'] = np.random.randn(1, 400) * 0.1  # Update W3 dimensions\n",
        "    parameters['b3'] = np.zeros((10, 1))\n",
        "    return parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35abd440",
      "metadata": {
        "id": "35abd440"
      },
      "outputs": [],
      "source": [
        "# Function to forward propagate through the network\n",
        "def forward_propagation(X, parameters):\n",
        "    W1, b1, W2, b2, W3, b3 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2'], parameters['W3'], parameters['b3']\n",
        "    Z1 = conv2d(X, W1, b1)\n",
        "    A1 = relu(Z1)\n",
        "    P1 = max_pooling(A1)\n",
        "    Z2 = conv2d(P1, W2, b2)\n",
        "    A2 = relu(Z2)\n",
        "    P2 = max_pooling(A2)\n",
        "    P2_flatten = P2.reshape(P2.shape[0] * P2.shape[1] * P2.shape[2], 1)\n",
        "    Z3 = np.dot(W3, P2_flatten) + b3\n",
        "    return softmax(Z3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32cdb252",
      "metadata": {
        "id": "32cdb252"
      },
      "outputs": [],
      "source": [
        "def backward_propagation(Y_pred, Y, caches, parameters):\n",
        "    m = Y.shape[1]\n",
        "    gradients = {}\n",
        "    dZ3 = Y_pred - Y\n",
        "    dW3 = 1 / m * np.dot(dZ3, caches['P2'].T)\n",
        "    db3 = 1 / m * np.sum(dZ3, axis=1, keepdims=True)\n",
        "    dZ2 = np.dot(parameters['W3'].T, dZ3) * relu_backward(caches['Z2'])\n",
        "    dW2 = 1 / m * np.dot(dZ2, caches['P1'].T)\n",
        "    db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dZ1 = np.dot(parameters['W2'].T, dZ2) * relu_backward(caches['Z1'])\n",
        "    dW1 = 1 / m * np.dot(dZ1, caches['X'].T)\n",
        "    db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    gradients['dW3'] = dW3\n",
        "    gradients['db3'] = db3\n",
        "    gradients['dW2'] = dW2\n",
        "    gradients['db2'] = db2\n",
        "    gradients['dW1'] = dW1\n",
        "    gradients['db1'] = db1\n",
        "\n",
        "    return gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cbcbdb0",
      "metadata": {
        "id": "1cbcbdb0"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    parameters['W3'] -= learning_rate * gradients['dW3']\n",
        "    parameters['b3'] -= learning_rate * gradients['db3']\n",
        "    parameters['W2'] -= learning_rate * gradients['dW2']\n",
        "    parameters['b2'] -= learning_rate * gradients['db2']\n",
        "    parameters['W1'] -= learning_rate * gradients['dW1']\n",
        "    parameters['b1'] -= learning_rate * gradients['db1']\n",
        "\n",
        "    return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc170e31",
      "metadata": {
        "id": "dc170e31"
      },
      "outputs": [],
      "source": [
        "# Function to compute cross-entropy loss\n",
        "def compute_loss(Y_pred, Y):\n",
        "    m = Y.shape[0]\n",
        "    logprobs = np.multiply(np.log(Y_pred), Y)\n",
        "    loss = -np.sum(logprobs) / m\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220eaa59",
      "metadata": {
        "id": "220eaa59"
      },
      "outputs": [],
      "source": [
        "def train_model(X_train, y_train, learning_rate=0.01, num_epochs=1):\n",
        "    parameters = initialize_parameters()\n",
        "    m = len(X_train)\n",
        "    epoch_ = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for i in range(m):\n",
        "            index = randint(0, m - 1)\n",
        "            X_batch, y_batch = X_train[index], y_train[index]\n",
        "            X_batch = X_batch.squeeze()  # Remove the singleton dimension\n",
        "            Y_pred = forward_propagation(X_batch, parameters)\n",
        "            Y = np.zeros((10, 1))\n",
        "            Y[y_batch] = 1\n",
        "            loss = compute_loss(Y_pred, Y)\n",
        "            epoch_loss += loss\n",
        "            if(i%50==0):\n",
        "                print(\"Epoch: \", epoch_)\n",
        "                epoch_+=1\n",
        "            # Backpropagation and parameter updates are not implemented for simplicity\n",
        "\n",
        "    return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9260f2f",
      "metadata": {
        "id": "b9260f2f"
      },
      "outputs": [],
      "source": [
        "# Function to predict labels for test data\n",
        "def predict(X_test, parameters):\n",
        "    m = len(X_test)\n",
        "    predictions = np.zeros(m)\n",
        "    for i in range(m):\n",
        "        Y_pred = forward_propagation(X_test[i], parameters)\n",
        "        predictions[i] = np.argmax(Y_pred)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "324b5306",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "324b5306",
        "outputId": "8fd4405b-89b9-43d9-e17d-0776ca075540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train:  1000\n",
            "X_test:  100\n",
            "y_train:  1000\n",
            "y_test:  100\n"
          ]
        }
      ],
      "source": [
        "# Load the MNIST dataset\n",
        "X_train, y_train, X_test, y_test = load_mnist()\n",
        "X_train, y_train, X_test, y_test = X_train[:1000], y_train[:1000], X_test[:100], y_test[:100]\n",
        "\n",
        "\n",
        "print(\"X_train: \", X_train.shape[0])\n",
        "print(\"X_test: \", X_test.shape[0])\n",
        "print(\"y_train: \", y_train.shape[0])\n",
        "print(\"y_test: \", y_test.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a3cfb6",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79a3cfb6",
        "outputId": "e29fee19-b000-46cd-f55f-f288b0bbb708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-f19000f9af00>:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  output[i, j, f] = np.sum(X_padded[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width, :] * kernel[:, :, :, f]) + bias[:, :, :, f]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0\n",
            "Epoch:  1\n",
            "Epoch:  2\n",
            "Epoch:  3\n",
            "Epoch:  4\n",
            "Epoch:  5\n",
            "Epoch:  6\n",
            "Epoch:  7\n",
            "Epoch:  8\n",
            "Epoch:  9\n",
            "Epoch:  10\n",
            "Epoch:  11\n",
            "Epoch:  12\n",
            "Epoch:  13\n",
            "Epoch:  14\n",
            "Epoch:  15\n",
            "Epoch:  16\n",
            "Epoch:  17\n",
            "Epoch:  18\n",
            "Epoch:  19\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "start = time()\n",
        "\n",
        "trained_parameters = train_model(X_train, y_train)\n",
        "\n",
        "stop = time()\n",
        "execution_time = stop-start\n",
        "total = X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b98fe4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7b98fe4",
        "outputId": "d4edfc0e-bf80-4c15-bcac-65ee0e6e263d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Execution time:  97.79597163200378\n",
            "      Throughput:  10.225370056784113\n"
          ]
        }
      ],
      "source": [
        "total = X_train.shape[0]\n",
        "print(\"  Execution time: \", execution_time)\n",
        "print(\"      Throughput: \", (total/execution_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac23ad2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ac23ad2",
        "outputId": "5f7e09c9-1992-459e-af26-89af145fd9f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-f19000f9af00>:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  output[i, j, f] = np.sum(X_padded[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width, :] * kernel[:, :, :, f]) + bias[:, :, :, f]\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "predictions = predict(X_test, trained_parameters)\n",
        "stop = time()\n",
        "execution_time = stop-start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c7db23d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c7db23d",
        "outputId": "4181a239-ea1e-4126-c743-0f8b02ecbc0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Execution time: 10.7606s\n",
            "      Throughput: 9.2932FPS\n"
          ]
        }
      ],
      "source": [
        "total = X_test.shape[0]\n",
        "print(\"  Execution time: {:.4f}s\".format(execution_time))\n",
        "print(\"      Throughput: {:.4f}FPS\".format(total/execution_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8715db00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8715db00",
        "outputId": "87bab114-ec9a-442e-c4f7-c35f1eb57871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.08\n"
          ]
        }
      ],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}